{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from transformers import TFBertModel\n",
    "answer_types=5\n",
    "model_dir = \"/Users/aashnabanerjee/Documents/Cortx/inference/code/bert-joint-baseline\"\n",
    "MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "from adamw_optimizer import AdamW\n",
    "train_filename = os.path.join(model_dir, \"nq-train.tfrecords-00000-of-00001\")\n",
    "# val_filename = os.path.join(model_dir, \"eval.tf_record\")\n",
    "val_filename2 = os.path.join(model_dir, \"op.tf_record\")\n",
    "num_train_examples = 4000\n",
    "epochs=2\n",
    "train_batch_size = 2\n",
    "batch_accumulation_size = 2\n",
    "init_learning_rate = 5e-5\n",
    "cyclic_learning_rate = True\n",
    "init_weight_decay_rate = 0.01\n",
    "num_warmup_steps = 0\n",
    "shuffle_buffer_size = 100000\n",
    "max_seq_length_for_training = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tf_record_file, seq_length, batch_size=1, shuffle_buffer_size=0, is_training=False):\n",
    "\n",
    "    if is_training:\n",
    "        features = {\n",
    "            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"start_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"end_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"answer_types\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    else:\n",
    "        features = {\n",
    "            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"token_map\": tf.io.FixedLenFeature([seq_length], tf.int64)\n",
    "        }        \n",
    "    def decode_record(record, features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.io.parse_single_example(record, features)\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "                        \n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.cast(t, tf.int32)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "    def select_data_from_record(record):\n",
    "        \n",
    "        x = {\n",
    "            'unique_ids': record['unique_ids'],\n",
    "            'input_ids': record['input_ids'],\n",
    "            'input_mask': record['input_mask'],\n",
    "            'segment_ids': record['segment_ids']\n",
    "        }\n",
    "        if not is_training:\n",
    "            x['token_map'] = record['token_map']\n",
    "\n",
    "        if is_training:\n",
    "            y = {\n",
    "                'start_positions': record['start_positions'],\n",
    "                'end_positions': record['end_positions'],\n",
    "                'answer_types': record['answer_types']\n",
    "            }\n",
    "\n",
    "            return (x, y)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(tf_record_file)\n",
    "    \n",
    "    dataset = dataset.map(lambda record: decode_record(record, features))\n",
    "    dataset = dataset.map(select_data_from_record)\n",
    "    \n",
    "    if shuffle_buffer_size > 0:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(train_filename,\n",
    "                    seq_length=512,\n",
    "                    batch_size=2,\n",
    "                    shuffle_buffer_size=10000,\n",
    "                    is_training=True\n",
    "                ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertMainLayer, TFBertPreTrainedModel, BertTokenizer\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "\n",
    "class TFNQModel(TFBertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        \n",
    "        TFBertPreTrainedModel.__init__(self, config, *inputs, **kwargs)  # explicit calls without super\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.backend = None\n",
    "        \n",
    "        self.seq_output_dropout = tf.keras.layers.Dropout(kwargs.get('seq_output_dropout_prob', 0.05))\n",
    "        self.pooled_output_dropout = tf.keras.layers.Dropout(kwargs.get('pooled_output_dropout_prob', 0.05))\n",
    "        \n",
    "        self.pos_classifier = tf.keras.layers.Dense(2,\n",
    "                                        kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                        name='pos_classifier')       \n",
    "\n",
    "        self.answer_type_classifier = tf.keras.layers.Dense(answer_types,\n",
    "                                        kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                        name='answer_type_classifier')         \n",
    "                \n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \n",
    "        # sequence / [CLS] outputs from original bert\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        sequence_output, pooled_output = outputs[0], outputs[1] \n",
    "        \n",
    "        # dropout\n",
    "        sequence_output = self.seq_output_dropout(sequence_output, training=kwargs.get('training', False))\n",
    "        pooled_output = self.pooled_output_dropout(pooled_output, training=kwargs.get('training', False))\n",
    "        \n",
    "        pos_logits = self.pos_classifier(sequence_output)  # shape = (batch_size, seq_len, 2)\n",
    "        start_pos_logits = pos_logits[:, :, 0]  # shape = (batch_size, seq_len)\n",
    "        end_pos_logits = pos_logits[:, :, 1]  # shape = (batch_size, seq_len)\n",
    "        \n",
    "        answer_type_logits = self.answer_type_classifier(pooled_output)  # shape = (batch_size, NB_ANSWER_TYPES)\n",
    "\n",
    "        outputs = (start_pos_logits, end_pos_logits, answer_type_logits)\n",
    "\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "def get_pretrained_model(MODEL_NAME):\n",
    "    \n",
    "    pretrained_path = os.path.join(model_dir, MODEL_NAME)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = TFNQModel.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    return tokenizer, model\n",
    "bert_tokenizer, bert_nq = get_pretrained_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(1, 8)\n",
      "(1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tf.constant(bert_tokenizer.encode(\"Hello, my dog is cute\"))[None, :]  # Batch size 1\n",
    "input_masks = tf.constant(0, shape=input_ids.shape)\n",
    "segment_ids = tf.constant(0, shape=input_ids.shape)\n",
    "\n",
    "# Actual inputs to model\n",
    "inputs = (input_ids, input_masks, segment_ids)\n",
    "\n",
    "# Outputs from bert_for_nq using backend_call()\n",
    "outputs = bert_nq(inputs)\n",
    "(start_pos_logits, end_pos_logits, answer_type_logits) = outputs\n",
    "print(start_pos_logits.shape)\n",
    "print(end_pos_logits.shape)\n",
    "print(answer_type_logits.shape)\n",
    "\n",
    "len(bert_nq.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(name):\n",
    "\n",
    "    loss = tf.keras.metrics.Mean(name=f'{name}_loss')\n",
    "    loss_start_pos = tf.keras.metrics.Mean(name=f'{name}_loss_start_pos')\n",
    "    loss_end_pos = tf.keras.metrics.Mean(name=f'{name}_loss_end_pos')\n",
    "    loss_ans_type = tf.keras.metrics.Mean(name=f'{name}_loss_ans_type')\n",
    "    \n",
    "    acc = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc')\n",
    "    acc_start_pos = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc_start_pos')\n",
    "    acc_end_pos = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc_end_pos')\n",
    "    acc_ans_type = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc_ans_type')\n",
    "    \n",
    "    return loss, loss_start_pos, loss_end_pos, loss_ans_type, acc, acc_start_pos, acc_end_pos, acc_ans_type\n",
    "\n",
    "train_loss, train_loss_start_pos, train_loss_end_pos, train_loss_ans_type, train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type = get_metrics(\"train\")\n",
    "valid_loss, valid_loss_start_pos, valid_loss_end_pos, valid_loss_ans_type, valid_acc, valid_acc_start_pos, valid_acc_end_pos, valid_acc_ans_type = get_metrics(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(nq_labels, nq_logits):\n",
    "    \n",
    "    (start_pos_labels, end_pos_labels, answer_type_labels) = nq_labels\n",
    "    (start_pos_logits, end_pos_logits, answer_type_logits) = nq_logits\n",
    "    \n",
    "    loss_start_pos = loss_object(start_pos_labels, start_pos_logits)\n",
    "    loss_end_pos = loss_object(end_pos_labels, end_pos_logits)\n",
    "    loss_ans_type = loss_object(answer_type_labels, answer_type_logits)\n",
    "    \n",
    "    loss_start_pos = tf.math.reduce_sum(loss_start_pos)\n",
    "    loss_end_pos = tf.math.reduce_sum(loss_end_pos)\n",
    "    loss_ans_type = tf.math.reduce_sum(loss_ans_type)\n",
    "    \n",
    "    loss = (loss_start_pos + loss_end_pos + loss_ans_type) / 3.0\n",
    "    \n",
    "    return loss, loss_start_pos, loss_end_pos, loss_ans_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.PolynomialDecay):\n",
    "    \n",
    "    def __init__(self,\n",
    "      initial_learning_rate,\n",
    "      decay_steps,\n",
    "      end_learning_rate=0.0001,\n",
    "      power=1.0,\n",
    "      cycle=False,\n",
    "      name=None,\n",
    "      num_warmup_steps=1000):\n",
    "        \n",
    "        # Since we have a custom __call__() method, we pass cycle=False when calling `super().__init__()` and\n",
    "        # in self.__call__(), we simply do `step = step % self.decay_steps` to have cyclic behavior.\n",
    "        super(CustomSchedule, self).__init__(initial_learning_rate, decay_steps, end_learning_rate, power, cycle=False, name=name)\n",
    "        \n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        \n",
    "        self.cycle = tf.constant(cycle, dtype=tf.bool)\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        \"\"\" `step` is actually the step index, starting at 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # For cyclic behavior\n",
    "        step = tf.cond(self.cycle and step >= self.decay_steps, lambda: step % self.decay_steps, lambda: step)\n",
    "        \n",
    "        learning_rate = super(CustomSchedule, self).__call__(step)\n",
    "\n",
    "        # Copy (including the comments) from original bert optimizer with minor change.\n",
    "        # Ref: https://github.com/google-research/bert/blob/master/optimization.py#L25\n",
    "        \n",
    "        # Implements linear warmup: if global_step < num_warmup_steps, the\n",
    "        # learning rate will be `global_step / num_warmup_steps * init_lr`.\n",
    "        if self.num_warmup_steps > 0:\n",
    "            \n",
    "            steps_int = tf.cast(step, tf.int32)\n",
    "            warmup_steps_int = tf.constant(self.num_warmup_steps, dtype=tf.int32)\n",
    "\n",
    "            steps_float = tf.cast(steps_int, tf.float32)\n",
    "            warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "            # The first training step has index (`step`) 0.\n",
    "            # The original code use `steps_float / warmup_steps_float`, which gives `warmup_percent_done` being 0,\n",
    "            # and causing `learning_rate` = 0, which is undesired.\n",
    "            # For this reason, we use `(steps_float + 1) / warmup_steps_float`.\n",
    "            # At `step = warmup_steps_float - 1`, i.e , at the `warmup_steps_float`-th step, \n",
    "            #`learning_rate` is `self.initial_learning_rate`.\n",
    "            warmup_percent_done = (steps_float + 1) / warmup_steps_float\n",
    "            \n",
    "            warmup_learning_rate = self.initial_learning_rate * warmup_percent_done\n",
    "\n",
    "            is_warmup = tf.cast(steps_int < warmup_steps_int, tf.float32)\n",
    "            learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "                        \n",
    "        return learning_rate\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "num_train_steps = int(epochs * num_train_examples / train_batch_size / batch_accumulation_size)\n",
    "\n",
    "learning_rate = CustomSchedule(initial_learning_rate=init_learning_rate,\n",
    "    decay_steps=num_train_steps,\n",
    "    end_learning_rate=init_learning_rate,\n",
    "    power=1.0,\n",
    "    cycle=cyclic_learning_rate,    \n",
    "    num_warmup_steps=num_warmup_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfnq_model_1/bert/embeddings/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/embeddings/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._0/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._1/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._2/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._3/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._4/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._5/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._6/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._7/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._8/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._9/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._10/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._11/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._12/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._13/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._14/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._15/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._16/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._17/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._18/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._19/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._20/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._21/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._22/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/attention/self/query/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/attention/self/key/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/attention/self/value/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/attention/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/attention/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/attention/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/intermediate/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/output/dense/bias:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/output/LayerNorm/gamma:0',\n",
       " 'tfnq_model_1/bert/encoder/layer_._23/output/LayerNorm/beta:0',\n",
       " 'tfnq_model_1/bert/pooler/dense/bias:0',\n",
       " 'tfnq_model_1/pos_classifier/bias:0',\n",
       " 'tfnq_model_1/answer_type_classifier/bias:0']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decay_var_list = []\n",
    "\n",
    "for i in range(len(bert_nq.trainable_variables)):\n",
    "    name = bert_nq.trainable_variables[i].name\n",
    "    if any(x in name for x in [\"LayerNorm\", \"layer_norm\", \"bias\"]):\n",
    "        decay_var_list.append(name)\n",
    "        \n",
    "decay_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# The hyperparameters are copied from AdamWeightDecayOptimizer in original bert code.\n",
    "# (https://github.com/google-research/bert/blob/master/optimization.py#L25)\n",
    "optimizer = AdamW(weight_decay=init_weight_decay_rate, learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay_var_list=decay_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signature = [\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_gradients(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels):\n",
    "    \n",
    "    nq_inputs = (input_ids, input_masks, segment_ids)\n",
    "    nq_labels = (start_pos_labels, end_pos_labels, answer_type_labels)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        nq_logits = bert_nq(nq_inputs, training=True)\n",
    "        loss, loss_start_pos, loss_end_pos, loss_ans_type = loss_function(nq_labels, nq_logits)\n",
    "                \n",
    "    gradients = tape.gradient(loss, bert_nq.trainable_variables)        \n",
    "        \n",
    "    (start_pos_logits, end_pos_logits, answer_type_logits) = nq_logits\n",
    "        \n",
    "    train_acc.update_state(start_pos_labels, start_pos_logits)\n",
    "    train_acc.update_state(end_pos_labels, end_pos_logits)\n",
    "    train_acc.update_state(answer_type_labels, answer_type_logits)\n",
    "\n",
    "    train_acc_start_pos.update_state(start_pos_labels, start_pos_logits)\n",
    "    train_acc_end_pos.update_state(end_pos_labels, end_pos_logits)\n",
    "    train_acc_ans_type.update_state(answer_type_labels, answer_type_logits)\n",
    "    \n",
    "    return loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type\n",
    "\n",
    "\n",
    "@tf.function(input_signature=input_signature)\n",
    "def train_step_simple(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels):\n",
    "\n",
    "    nb_examples = tf.math.reduce_sum(tf.cast(tf.math.not_equal(start_pos_labels, -2), tf.int32))\n",
    "    \n",
    "    loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type = get_loss_and_gradients(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels)\n",
    "    \n",
    "    average_loss = tf.math.divide(loss, tf.cast(nb_examples, tf.float32))\n",
    "    average_gradients = [tf.divide(x, tf.cast(nb_examples, tf.float32)) for x in gradients]\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, bert_nq.trainable_variables))\n",
    "\n",
    "    average_loss_start_pos = tf.math.divide(loss_start_pos, tf.cast(nb_examples, tf.float32))\n",
    "    average_loss_end_pos = tf.math.divide(loss_end_pos, tf.cast(nb_examples, tf.float32))\n",
    "    average_loss_ans_type = tf.math.divide(loss_ans_type, tf.cast(nb_examples, tf.float32))\n",
    "    \n",
    "    train_loss(average_loss)\n",
    "    train_loss_start_pos(average_loss_start_pos)\n",
    "    train_loss_end_pos(average_loss_end_pos)\n",
    "    train_loss_ans_type(average_loss_ans_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=input_signature)\n",
    "def train_step_with_batch_accumulation(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels):\n",
    "\n",
    "    # This gets None! (probably due to input_signature)\n",
    "    # batch_size = input_ids.shape[0]\n",
    "    \n",
    "    # Try this.\n",
    "    nb_examples = tf.math.reduce_sum(tf.cast(tf.math.not_equal(start_pos_labels, -2), tf.int32))\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_loss_start_pos = 0.0\n",
    "    total_loss_end_pos = 0.0\n",
    "    total_loss_ans_type = 0.0\n",
    "    \n",
    "    total_gradients = [tf.constant(0, shape=x.shape, dtype=tf.float32) for x in bert_nq.trainable_variables]        \n",
    "    ### total_gradients_sparse = [tf.IndexedSlices(values=tf.constant(0.0, shape=[1] + x.shape.as_list()[1:]), indices=tf.constant([0], dtype=tf.int32), dense_shape=x.shape.as_list()) for x in bert_nq.trainable_variables]        \n",
    "\n",
    "    for idx in tf.range(batch_accumulation_size):   \n",
    "        start_idx = train_batch_size * idx\n",
    "        end_idx = train_batch_size * (idx + 1)\n",
    "        \n",
    "        if start_idx >= nb_examples:\n",
    "            break\n",
    "\n",
    "        (input_ids_mini, input_masks_mini, segment_ids_mini) = (input_ids[start_idx:end_idx], input_masks[start_idx:end_idx], segment_ids[start_idx:end_idx])\n",
    "        (start_pos_labels_mini, end_pos_labels_mini, answer_type_labels_mini) = (start_pos_labels[start_idx:end_idx], end_pos_labels[start_idx:end_idx], answer_type_labels[start_idx:end_idx])\n",
    "        \n",
    "        loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type = get_loss_and_gradients(input_ids_mini, input_masks_mini, segment_ids_mini, start_pos_labels_mini, end_pos_labels_mini, answer_type_labels_mini)\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_loss_start_pos += loss_start_pos\n",
    "        total_loss_end_pos += loss_end_pos\n",
    "        total_loss_ans_type += loss_ans_type\n",
    "        \n",
    "        total_gradients = [x + y for x, y in zip(total_gradients, gradients)]  \n",
    "    \n",
    "    average_loss = tf.math.divide(total_loss, tf.cast(nb_examples, tf.float32))        \n",
    "    average_gradients = [tf.divide(x, tf.cast(nb_examples, tf.float32)) for x in total_gradients]\n",
    "    ### average_gradients_sparse = [tf.scalar_mul(tf.divide(1.0, tf.cast(nb_examples, tf.float32)), x) for x in total_gradients_sparse]\n",
    "    \n",
    "    optimizer.apply_gradients(zip(average_gradients, bert_nq.trainable_variables))\n",
    "    ### optimizer.apply_gradients(zip(average_gradients_sparse, bert_nq.trainable_variables))\n",
    "\n",
    "    average_loss_start_pos = tf.math.divide(total_loss_start_pos, tf.cast(nb_examples, tf.float32))\n",
    "    average_loss_end_pos = tf.math.divide(total_loss_end_pos, tf.cast(nb_examples, tf.float32))\n",
    "    average_loss_ans_type = tf.math.divide(total_loss_ans_type, tf.cast(nb_examples, tf.float32))    \n",
    "    \n",
    "    train_loss(average_loss)\n",
    "    train_loss_start_pos(average_loss_start_pos)\n",
    "    train_loss_end_pos(average_loss_end_pos)\n",
    "    train_loss_ans_type(average_loss_ans_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint not found. Train BertNQ from scratch\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = model_dir\n",
    "ckpt = tf.train.Checkpoint(model=bert_nq)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10000)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print (f'Latest BertNQ checkpoint restored -- Model trained for {last_epoch} epochs')\n",
    "else:\n",
    "    print('Checkpoint not found. Train BertNQ from scratch')\n",
    "    last_epoch = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aashnabanerjee/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 1 | Elapsed Time 108.805175\n",
      "Loss 4.870275 | Loss_S 6.320004 | Loss_E 6.687675 | Loss_T 1.603146\n",
      " Acc 0.000000 |  Acc_S 0.000000 |  Acc_E 0.000000 |  Acc_T 0.000000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 2 | Elapsed Time 62.339668\n",
      "Loss 4.520985 | Loss_S 5.797989 | Loss_E 6.140835 | Loss_T 1.624130\n",
      " Acc 0.041667 |  Acc_S 0.125000 |  Acc_E 0.000000 |  Acc_T 0.000000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 3 | Elapsed Time 65.431683\n",
      "Loss 4.309756 | Loss_S 5.542434 | Loss_E 5.868500 | Loss_T 1.518332\n",
      " Acc 0.138889 |  Acc_S 0.166667 |  Acc_E 0.000000 |  Acc_T 0.250000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 4 | Elapsed Time 82.820423\n",
      "Loss 3.970361 | Loss_S 5.062921 | Loss_E 5.401593 | Loss_T 1.446568\n",
      " Acc 0.166667 |  Acc_S 0.187500 |  Acc_E 0.000000 |  Acc_T 0.312500\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 5 | Elapsed Time 85.764724\n",
      "Loss 3.837545 | Loss_S 4.932735 | Loss_E 5.204633 | Loss_T 1.375266\n",
      " Acc 0.183333 |  Acc_S 0.150000 |  Acc_E 0.000000 |  Acc_T 0.400000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 6 | Elapsed Time 72.227272\n",
      "Loss 3.556446 | Loss_S 4.525826 | Loss_E 4.783184 | Loss_T 1.360329\n",
      " Acc 0.194444 |  Acc_S 0.166667 |  Acc_E 0.041667 |  Acc_T 0.375000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 7 | Elapsed Time 70.154842\n",
      "Loss 3.533755 | Loss_S 4.539224 | Loss_E 4.758274 | Loss_T 1.303767\n",
      " Acc 0.190476 |  Acc_S 0.142857 |  Acc_E 0.035714 |  Acc_T 0.392857\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 8 | Elapsed Time 73.097069\n",
      "Loss 3.533636 | Loss_S 4.541758 | Loss_E 4.768587 | Loss_T 1.290564\n",
      " Acc 0.197917 |  Acc_S 0.187500 |  Acc_E 0.031250 |  Acc_T 0.375000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 9 | Elapsed Time 74.223214\n",
      "Loss 3.450906 | Loss_S 4.399682 | Loss_E 4.667976 | Loss_T 1.285059\n",
      " Acc 0.194444 |  Acc_S 0.166667 |  Acc_E 0.027778 |  Acc_T 0.388889\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 10 | Elapsed Time 73.502801\n",
      "Loss 3.260291 | Loss_S 4.148208 | Loss_E 4.380745 | Loss_T 1.251919\n",
      " Acc 0.208333 |  Acc_S 0.175000 |  Acc_E 0.075000 |  Acc_T 0.375000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 11 | Elapsed Time 60.994643\n",
      "Loss 3.098918 | Loss_S 3.948268 | Loss_E 4.126792 | Loss_T 1.221693\n",
      " Acc 0.204545 |  Acc_S 0.159091 |  Acc_E 0.068182 |  Acc_T 0.386364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 12 | Elapsed Time 58.300465\n",
      "Loss 2.910973 | Loss_S 3.694818 | Loss_E 3.864400 | Loss_T 1.173702\n",
      " Acc 0.236111 |  Acc_S 0.187500 |  Acc_E 0.083333 |  Acc_T 0.437500\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 13 | Elapsed Time 63.204592\n",
      "Loss 2.940047 | Loss_S 3.766210 | Loss_E 3.870316 | Loss_T 1.183614\n",
      " Acc 0.243590 |  Acc_S 0.192308 |  Acc_E 0.096154 |  Acc_T 0.442308\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 14 | Elapsed Time 63.43412\n",
      "Loss 2.947984 | Loss_S 3.789836 | Loss_E 3.878684 | Loss_T 1.175433\n",
      " Acc 0.250000 |  Acc_S 0.196429 |  Acc_E 0.107143 |  Acc_T 0.446429\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 15 | Elapsed Time 64.407254\n",
      "Loss 3.030046 | Loss_S 3.876591 | Loss_E 3.945088 | Loss_T 1.268457\n",
      " Acc 0.233333 |  Acc_S 0.183333 |  Acc_E 0.100000 |  Acc_T 0.416667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 16 | Elapsed Time 60.659925\n",
      "Loss 3.027145 | Loss_S 3.872123 | Loss_E 3.924785 | Loss_T 1.284528\n",
      " Acc 0.223958 |  Acc_S 0.171875 |  Acc_E 0.093750 |  Acc_T 0.406250\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 17 | Elapsed Time 64.682808\n",
      "Loss 2.966744 | Loss_S 3.790327 | Loss_E 3.835707 | Loss_T 1.274197\n",
      " Acc 0.230392 |  Acc_S 0.191176 |  Acc_E 0.102941 |  Acc_T 0.397059\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 18 | Elapsed Time 63.335872\n",
      "Loss 2.929015 | Loss_S 3.746504 | Loss_E 3.803278 | Loss_T 1.237262\n",
      " Acc 0.236111 |  Acc_S 0.180556 |  Acc_E 0.097222 |  Acc_T 0.430556\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 19 | Elapsed Time 62.364373\n",
      "Loss 2.894155 | Loss_S 3.678277 | Loss_E 3.782876 | Loss_T 1.221313\n",
      " Acc 0.236842 |  Acc_S 0.184211 |  Acc_E 0.105263 |  Acc_T 0.421053\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 20 | Elapsed Time 61.980844\n",
      "Loss 2.906861 | Loss_S 3.644154 | Loss_E 3.815763 | Loss_T 1.260666\n",
      " Acc 0.241667 |  Acc_S 0.187500 |  Acc_E 0.112500 |  Acc_T 0.425000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 21 | Elapsed Time 60.979287\n",
      "Loss 2.956809 | Loss_S 3.730620 | Loss_E 3.884042 | Loss_T 1.255763\n",
      " Acc 0.242063 |  Acc_S 0.178571 |  Acc_E 0.119048 |  Acc_T 0.428571\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 22 | Elapsed Time 69.151446\n",
      "Loss 2.947997 | Loss_S 3.709828 | Loss_E 3.877788 | Loss_T 1.256376\n",
      " Acc 0.246212 |  Acc_S 0.193182 |  Acc_E 0.125000 |  Acc_T 0.420455\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 23 | Elapsed Time 961.919334\n",
      "Loss 2.952171 | Loss_S 3.720833 | Loss_E 3.901053 | Loss_T 1.234627\n",
      " Acc 0.250000 |  Acc_S 0.195652 |  Acc_E 0.119565 |  Acc_T 0.434783\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Batch 24 | Elapsed Time 111.11672\n",
      "Loss 2.911280 | Loss_S 3.666476 | Loss_E 3.825624 | Loss_T 1.241739\n",
      " Acc 0.250000 |  Acc_S 0.208333 |  Acc_E 0.125000 |  Acc_T 0.416667\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "if batch_accumulation_size > 1:\n",
    "    train_step = train_step_with_batch_accumulation\n",
    "\n",
    "\n",
    "train_start_time = datetime.datetime.now()\n",
    "\n",
    "epochs = epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_dataset = get_dataset(\n",
    "        train_filename,\n",
    "        max_seq_length_for_training,\n",
    "        batch_accumulation_size * train_batch_size,\n",
    "        shuffle_buffer_size,\n",
    "        is_training=True\n",
    "    )     \n",
    "    train_loss.reset_states()\n",
    "    train_loss_start_pos.reset_states()\n",
    "    train_loss_end_pos.reset_states()\n",
    "    train_loss_ans_type.reset_states()    \n",
    "    \n",
    "    train_acc.reset_states()\n",
    "    train_acc_start_pos.reset_states()\n",
    "    train_acc_end_pos.reset_states()\n",
    "    train_acc_ans_type.reset_states()\n",
    "    \n",
    "    epoch_start_time = datetime.datetime.now()\n",
    "    \n",
    "    for (batch_idx, (features, targets)) in enumerate(train_dataset):\n",
    "                \n",
    "        \n",
    "        (input_ids, input_masks, segment_ids) = (features['input_ids'], features['input_mask'], features['segment_ids'])\n",
    "        (start_pos_labels, end_pos_labels, answer_type_labels) = (targets['start_positions'], targets['end_positions'], targets['answer_types'])\n",
    "        batch_start_time = datetime.datetime.now()\n",
    "        \n",
    "        train_step(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels)\n",
    "\n",
    "        batch_end_time = datetime.datetime.now()\n",
    "        batch_elapsed_time = (batch_end_time - batch_start_time).total_seconds()\n",
    "        \n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print('Epoch {} | Batch {} | Elapsed Time {}'.format(\n",
    "                epoch + 1,\n",
    "                batch_idx + 1,\n",
    "                batch_elapsed_time\n",
    "            ))\n",
    "            print('Loss {:.6f} | Loss_S {:.6f} | Loss_E {:.6f} | Loss_T {:.6f}'.format(\n",
    "                train_loss.result(),\n",
    "                train_loss_start_pos.result(),\n",
    "                train_loss_end_pos.result(),\n",
    "                train_loss_ans_type.result()\n",
    "            ))\n",
    "            print(' Acc {:.6f} |  Acc_S {:.6f} |  Acc_E {:.6f} |  Acc_T {:.6f}'.format(\n",
    "                train_acc.result(),train_acc_start_pos.result(),\n",
    "                train_acc_end_pos.result(),\n",
    "                train_acc_ans_type.result()\n",
    "            ))\n",
    "            print(\"-\" * 100)\n",
    "       \n",
    "    epoch_end_time = datetime.datetime.now()\n",
    "    epoch_elapsed_time = (epoch_end_time - epoch_start_time).total_seconds()\n",
    "            \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        \n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "        \n",
    "        print('\\nEpoch {}'.format(epoch + 1))\n",
    "        print('Loss {:.6f} | Loss_S {:.6f} | Loss_E {:.6f} | Loss_T {:.6f}'.format(\n",
    "            train_loss.result(),\n",
    "            train_loss_start_pos.result(),\n",
    "            train_loss_end_pos.result(),\n",
    "            train_loss_ans_type.result()\n",
    "        ))\n",
    "        print(' Acc {:.6f} |  Acc_S {:.6f} |  Acc_E {:.6f} |  Acc_T {:.6f}'.format(\n",
    "            train_acc.result(),\n",
    "            train_acc_start_pos.result(),\n",
    "            train_acc_end_pos.result(),\n",
    "            train_acc_ans_type.result()\n",
    "        ))\n",
    "\n",
    "    print('\\nTime taken for 1 epoch: {} secs\\n'.format(epoch_elapsed_time))\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
