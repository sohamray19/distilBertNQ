{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "disttilBertNQ",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb2NoI-cEQnn",
        "colab_type": "code",
        "outputId": "3c1a6d97-843c-4a53-88ab-e0579ac303b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from transformers import TFDistilBertModel, DistilBertTokenizer, TFDistilBertMainLayer, TFDistilBertPreTrainedModel\n",
        "from transformers.modeling_tf_utils import get_initializer\n",
        "from generate_predictions import get_prediction_json\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from adamw_optimizer import AdamW\n",
        "epochs=5\n",
        "batch_size = 2\n",
        "init_learning_rate = 5e-5\n",
        "init_weight_decay_rate = 0.01\n",
        "num_warmup_steps = 0\n",
        "shuffle_buffer_size = 100000\n",
        "best_indexes = 20\n",
        "answer_types = 5\n",
        "\n",
        "\n",
        "def decode_record(record, x):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.io.parse_single_example(record, x)\n",
        "    for name in list(example.keys()):\n",
        "        \"\"\"Type conversion for compatibilty\"\"\"\n",
        "        t = example[name]\n",
        "        if t.dtype == tf.int64:\n",
        "            t = tf.cast(t, tf.int32)\n",
        "        example[name] = t\n",
        "    return example\n",
        "\n",
        "def read_train_record(tf_record_file, shuffle_buffer_size, batch_size=1):\n",
        "    \"\"\"\n",
        "    Reads tf records into a MapDataset for training\n",
        "\n",
        "    Parameters: tf_record file and hyperparameters\n",
        "\n",
        "    Returns: Training dataset\n",
        "    \"\"\"\n",
        "    def x_map(record):\n",
        "        return ({\n",
        "                    'unique_ids': record['unique_ids'],\n",
        "                    'input_ids': record['input_ids'],\n",
        "                    'input_mask': record['input_mask'],\n",
        "                    'segment_ids': record['segment_ids'],\n",
        "        }, {\n",
        "            'start_positions': record['start_positions'],\n",
        "            'end_positions': record['end_positions'],\n",
        "            'answer_types': record['answer_types']\n",
        "        })\n",
        "\n",
        "    x = {\n",
        "         \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
        "         \"input_ids\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "         \"input_mask\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "         \"segment_ids\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "         \"start_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
        "         \"end_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
        "         \"answer_types\": tf.io.FixedLenFeature([], tf.int64)\n",
        "         }\n",
        "\n",
        "    #  read dataset from record into examples\n",
        "    dataset = tf.data.TFRecordDataset(tf_record_file).map(lambda record: decode_record(record, x))\n",
        "    #  shuffle\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size) if shuffle_buffer_size != 0 else dataset\n",
        "    # create batches\n",
        "    dataset = dataset.batch(batch_size) if batch_size != 0 else dataset\n",
        "    #  map dataset to features dictionary for ease of access\n",
        "    dataset = dataset.map(x_map)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def read_val_record(tf_record_file, shuffle_buffer_size, batch_size=1):\n",
        "    \"\"\"\n",
        "    Reads tf records into a MapDataset for validation\n",
        "\n",
        "    Parameters: tf_record file and hyperparameters\n",
        "\n",
        "    Returns: Validation dataset\n",
        "    \"\"\"\n",
        "    def x_map(record):\n",
        "        return ({\n",
        "                    'unique_ids': record['unique_ids'],\n",
        "                    'input_ids': record['input_ids'],\n",
        "                    'input_mask': record['input_mask'],\n",
        "                    'segment_ids': record['segment_ids'],\n",
        "                    'token_map': record['token_map']\n",
        "\n",
        "        })\n",
        "\n",
        "    x = {\"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
        "         \"input_ids\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "         \"input_mask\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "         \"segment_ids\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "         \"token_map\": tf.io.FixedLenFeature([512], tf.int64)\n",
        "         }\n",
        "\n",
        "    #  read dataset from record into examples\n",
        "    dataset = tf.data.TFRecordDataset(tf_record_file).map(lambda record: decode_record(record, x))\n",
        "    #  shuffle\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size) if shuffle_buffer_size != 0 else dataset\n",
        "    # create batches\n",
        "    dataset = dataset.batch(batch_size) if batch_size != 0 else dataset\n",
        "    #  map dataset to features dictionary for ease of access\n",
        "    dataset = dataset.map(x_map)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "class TFNQModel(TFDistilBertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        \"\"\"Initializes model\"\"\"\n",
        "        # initialize pretrained model\n",
        "        TFDistilBertPreTrainedModel.__init__(self, config, *inputs, **kwargs)\n",
        "\n",
        "        # set backend as DistilBert\n",
        "        self.backend = TFDistilBertMainLayer(config, name=\"distilbert\")\n",
        "\n",
        "        #initialize dropout layers\n",
        "        self.seq_output_dropout = tf.keras.layers.Dropout(kwargs.get('seq_output_dropout_prob', 0.05))\n",
        "        self.pooled_output_dropout = tf.keras.layers.Dropout(kwargs.get('pooled_output_dropout_prob', 0.05))\n",
        "\n",
        "        #set up classifiers on BERT outputs to give us start and end pos tags, as well as an answer type tag\n",
        "        self.pos_classifier = tf.keras.layers.Dense(2,\n",
        "                                                    kernel_initializer=get_initializer(config.initializer_range),\n",
        "                                                    name='pos_classifier')\n",
        "\n",
        "        self.answer_type_classifier = tf.keras.layers.Dense(answer_types,\n",
        "                                                            kernel_initializer=get_initializer(\n",
        "                                                                config.initializer_range),\n",
        "                                                            name='answer_type_classifier')\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Invoked when model called to return logits\n",
        "\n",
        "        Returns: logits for start token, end token and answer type\n",
        "\n",
        "        \"\"\"\n",
        "        inputs = inputs[:2] if isinstance(inputs, tuple) else inputs\n",
        "        outputs = self.backend(inputs, **kwargs)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = sequence_output[:, 0, :]\n",
        "        #dropout fro both outputs\n",
        "        sequence_output = self.seq_output_dropout(sequence_output, training=kwargs.get('training', False))\n",
        "        pooled_output = self.pooled_output_dropout(pooled_output, training=kwargs.get('training', False))\n",
        "        #splitting into start and end after passing throught classifier built on top of bert\n",
        "        pos_logits = self.pos_classifier(sequence_output)\n",
        "        start_pos_logits = pos_logits[:, :, 0]\n",
        "        end_pos_logits = pos_logits[:, :, 1]\n",
        "\n",
        "        answer_type_logits = self.answer_type_classifier(pooled_output)\n",
        "\n",
        "        outputs = (start_pos_logits, end_pos_logits, answer_type_logits)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def initialize_acc():\n",
        "    '''Initialize accuracy metrics using Sparse TopK categorical accuracy'''\n",
        "    start_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
        "    end_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
        "    ans_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
        "    total_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
        "    return total_acc, start_acc, end_acc, ans_acc\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.PolynomialDecay):\n",
        "    \n",
        "    def __init__(self,\n",
        "      initial_learning_rate,\n",
        "      decay_steps,\n",
        "      end_learning_rate=0.0001,\n",
        "      power=1.0,\n",
        "      cycle=False,\n",
        "      name=None,\n",
        "      num_warmup_steps=1000):\n",
        "        \n",
        "        # Since we have a custom __call__() method, we pass cycle=False when calling `super().__init__()` and\n",
        "        # in self.__call__(), we simply do `step = step % self.decay_steps` to have cyclic behavior.\n",
        "        super(CustomSchedule, self).__init__(initial_learning_rate, decay_steps, end_learning_rate, power, cycle=False, name=name)\n",
        "        \n",
        "        self.num_warmup_steps = num_warmup_steps\n",
        "        \n",
        "        self.cycle = tf.constant(cycle, dtype=tf.bool)\n",
        "        \n",
        "    def __call__(self, step):\n",
        "        \"\"\" `step` is actually the step index, starting at 0.\n",
        "        \"\"\"\n",
        "        \n",
        "        # For cyclic behavior\n",
        "        step = tf.cond(self.cycle and step >= self.decay_steps, lambda: step % self.decay_steps, lambda: step)\n",
        "        \n",
        "        learning_rate = super(CustomSchedule, self).__call__(step)\n",
        "\n",
        "        # Copy (including the comments) from original bert optimizer with minor change.\n",
        "        # Ref: https://github.com/google-research/bert/blob/master/optimization.py#L25\n",
        "        \n",
        "        # Implements linear warmup: if global_step < num_warmup_steps, the\n",
        "        # learning rate will be `global_step / num_warmup_steps * init_lr`.\n",
        "        if self.num_warmup_steps > 0:\n",
        "            \n",
        "            steps_int = tf.cast(step, tf.int32)\n",
        "            warmup_steps_int = tf.constant(self.num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "            steps_float = tf.cast(steps_int, tf.float32)\n",
        "            warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "            # The first training step has index (`step`) 0.\n",
        "            # The original code use `steps_float / warmup_steps_float`, which gives `warmup_percent_done` being 0,\n",
        "            # and causing `learning_rate` = 0, which is undesired.\n",
        "            # For this reason, we use `(steps_float + 1) / warmup_steps_float`.\n",
        "            # At `step = warmup_steps_float - 1`, i.e , at the `warmup_steps_float`-th step, \n",
        "            #`learning_rate` is `self.initial_learning_rate`.\n",
        "            warmup_percent_done = (steps_float + 1) / warmup_steps_float\n",
        "            \n",
        "            warmup_learning_rate = self.initial_learning_rate * warmup_percent_done\n",
        "\n",
        "            is_warmup = tf.cast(steps_int < warmup_steps_int, tf.float32)\n",
        "            learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "                        \n",
        "        return learning_rate\n",
        " \n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from transformers import create_optimizer as co\n",
        "def create_optimizer(distilBert):\n",
        "    \"\"\"Initializes LAMB optimizer\"\"\"\n",
        "    num_train_steps = int(epochs * 50000 / batch_size)\n",
        "    # return co(init_learning_rate, num_train_steps, 1000, end_lr=0.0, optimizer_type='adamw')\n",
        "    #AdamW optimizer, similar to what Google uses from tensorflow addons library\n",
        "    #     return tfa.optimizers.AdamW(weight_decay=FLAGS.init_weight_decay_rate, learning_rate=FLAGS.init_learning_rate, beta_1=0.9, beta_2=0.999,\n",
        "    #                       epsilon=1e-6)\n",
        "\n",
        "    \n",
        "    # learning rate scheduler\n",
        "    schedule = CustomSchedule(initial_learning_rate=init_learning_rate,\n",
        "    decay_steps=num_train_steps,\n",
        "    end_learning_rate=init_learning_rate,\n",
        "    power=1.0,\n",
        "    cycle=True,    \n",
        "    num_warmup_steps=0\n",
        "    )\n",
        "    \n",
        "    # decay_var_list = []\n",
        "\n",
        "    # for i in range(len(distilBert.trainable_variables)):\n",
        "    #     name = distilBert.trainable_variables[i].name\n",
        "    #     if any(x in name for x in [\"LayerNorm\", \"layer_norm\", \"bias\"]):\n",
        "    #         decay_var_list.append(name)\n",
        "    \n",
        "    # return AdamW(weight_decay=init_weight_decay_rate, learning_rate=schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay_var_list=decay_var_list)\n",
        "    \n",
        "    \n",
        "    # LAMB optimizer, known for training BERT super fast (find it at https://arxiv.org/abs/1904.00962 )\n",
        "    return tfa.optimizers.LAMB(\n",
        "        learning_rate=schedule,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-06,\n",
        "        weight_decay_rate=init_weight_decay_rate,\n",
        "        exclude_from_weight_decay = [\"LayerNorm\", \"layer_norm\", \"bias\"],\n",
        "        name='LAMB'\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_loss(positions, logits):\n",
        "    '''Finds loss between logits and labels'''\n",
        "\n",
        "    # the way google defines loss in their bert-joint-baseline paper\n",
        "    one_hot_positions = tf.one_hot(positions, depth=512, dtype=tf.float32)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    loss = -tf.reduce_mean(tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "\n",
        "    return loss\n",
        "\n",
        "    # using sparse categorical cross entropy (for both answer types and start and end tokens)\n",
        "\n",
        "    # loss_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # return tf.math.reduce_sum(loss_(positions, logits))\n",
        "\n",
        "\n",
        "def compute_label_loss(labels, logits):\n",
        "    '''Find loss for answer type labels'''\n",
        "    # the way google defines loss in their bert-joint-baseline paper\n",
        "    one_hot_labels = tf.one_hot(labels, depth=answer_types, dtype=tf.float32)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    loss = -tf.reduce_mean(\n",
        "                tf.reduce_sum(one_hot_labels * log_probs, axis=-1))\n",
        "    return loss\n",
        "    # loss_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # return tf.math.reduce_sum(loss_(labels, logits))\n",
        "\n",
        "def compute_gradient(distilBert, input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels,train_acc, train_acc_start_pos,\n",
        "                                              train_acc_end_pos, train_acc_ans_type ):\n",
        "    '''\n",
        "    Computes gradient based on averaged loss from start token, end token and answer type\n",
        "\n",
        "    Inputs: features (x), labels (y), accuracy metrics\n",
        "\n",
        "    Returns: Gradients, accuracy\n",
        "    '''\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        #find loss for all three outputs and average it to find total loss\n",
        "        (start_pos_logits, end_pos_logits, answer_type_logits) = distilBert((input_ids, input_masks, segment_ids),\n",
        "                                                                         training=True)\n",
        "        loss_start_pos = compute_loss(start_pos_labels, start_pos_logits)\n",
        "        loss_end_pos = compute_loss(end_pos_labels, end_pos_logits)\n",
        "        loss_ans_type = compute_label_loss(answer_type_labels, answer_type_logits)\n",
        "        total_loss = (loss_start_pos + loss_end_pos + loss_ans_type) / 3.0\n",
        "\n",
        "    gradients = tape.gradient(total_loss, distilBert.trainable_variables)\n",
        "\n",
        "    train_acc.update_state(start_pos_labels, start_pos_logits)\n",
        "    train_acc.update_state(end_pos_labels, end_pos_logits)\n",
        "    train_acc.update_state(answer_type_labels, answer_type_logits)\n",
        "    train_acc_start_pos.update_state(start_pos_labels, start_pos_logits)\n",
        "    train_acc_end_pos.update_state(end_pos_labels, end_pos_logits)\n",
        "    train_acc_ans_type.update_state(answer_type_labels, answer_type_logits)\n",
        "\n",
        "    acc = (train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type)\n",
        "\n",
        "    return gradients, acc\n",
        "\n",
        "\n",
        "def checkpt(distilBert, checkpoint_path):\n",
        "    \"\"\"Reads checkpoint if present and returns checkpoint manager to store checkpoints if required\"\"\"\n",
        "    ckpt = tf.train.Checkpoint(model=distilBert)\n",
        "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=100)\n",
        "    #restore latest checkpoint if present\n",
        "    if ckpt_manager.latest_checkpoint:\n",
        "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "        last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
        "        print(last_epoch)\n",
        "        print(\"Latest checkpoint restored\")\n",
        "    else:\n",
        "        print(\"No checkpoint found\")\n",
        "    return ckpt_manager\n",
        "    \n",
        "\n",
        "def train(distilBert, optimizer, train_dataset, ckpt_manager, train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type):\n",
        "    \"\"\"\n",
        "    Trains Model as per configurations\n",
        "\n",
        "    Parameters: model, dataset, checkpoint manager, and metrics\n",
        "\n",
        "    Returns: nothing but stores checkpoints\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        #reset metrics at every epoch\n",
        "        train_acc.reset_states()\n",
        "        train_acc_start_pos.reset_states()\n",
        "        train_acc_end_pos.reset_states()\n",
        "        train_acc_ans_type.reset_states()\n",
        "\n",
        "        for (instance, (x, y)) in enumerate(train_dataset):\n",
        "            # if instance<50000: continue\n",
        "            # if instance==50001: print(\"starting from 50001\")\n",
        "            if instance % 25000 == 0: ckpt_manager.save()\n",
        "            #generate x and y \n",
        "            input_ids, input_masks, segment_ids = (x['input_ids'], x['input_mask'], x['segment_ids'])\n",
        "            start_pos_labels, end_pos_labels, answer_type_labels = (\n",
        "            y['start_positions'], y['end_positions'], y['answer_types'])\n",
        "\n",
        "            #generate gradients and accuracy\n",
        "            gradients, acc = compute_gradient(distilBert, input_ids, input_masks, segment_ids, start_pos_labels,\n",
        "                                    end_pos_labels, answer_type_labels, train_acc, train_acc_start_pos,\n",
        "                                    train_acc_end_pos, train_acc_ans_type)\n",
        "\n",
        "            #apply gradients\n",
        "\n",
        "            # syntax for tensorflow optimizer\n",
        "            # optimizer.apply_gradients(zip(gradients, distilBert.trainable_variables))\n",
        "\n",
        "            # syntax for huggingface optimizer\n",
        "            optimizer.apply_gradients([(gradients[i],dtv) for i,dtv in enumerate(distilBert.trainable_variables)])\n",
        "\n",
        "            #print accuracy\n",
        "            (train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type) = acc\n",
        "\n",
        "            if (instance + 1) % 100 == 0:\n",
        "                print('Epoch {}, Instances processed {}'.format(epoch + 1,instance + 1, ))\n",
        "\n",
        "                print('Accuracy: Overall {:.6f}, Start Token {:.4f}, End Token {:.4f}, Answer Type {:.4f} '.format(train_acc.result(), \n",
        "                                train_acc_start_pos.result(), train_acc_end_pos.result(), train_acc_ans_type.result()))\n",
        "            \n",
        "\n",
        "                print(\"-\" * 100)\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_manager.save()))\n",
        "\n",
        "            print('Accuracy: Overall {:.6f}, Start Token {:.4f}, End Token {:.4f}, Answer Type {:.4f} '.format(train_acc.result(), \n",
        "                            train_acc_start_pos.result(), train_acc_end_pos.result(), train_acc_ans_type.result()))\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #retrieve datasets\n",
        "    training_mode=False\n",
        "    use_chkpt=True\n",
        "    train_file = \"/content/drive/My Drive/Colab Notebooks/nq-train.tfrecords-00000-of-00001\"\n",
        "    val_file = \"/content/drive/My Drive/Colab Notebooks/eval_dev.tf_record\"\n",
        "    pred_file = \"/content/drive/My Drive/Colab Notebooks/nq-dev-??.jsonl.gz\"\n",
        "    json_output_path = \"/content/drive/My Drive/Colab Notebooks/predictions.json\"\n",
        "    checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/checkpoints/\"\n",
        "    if training_mode: train_dataset = read_train_record(train_file, shuffle_buffer_size,batch_size)\n",
        "    else: val_dataset = read_val_record(val_file, shuffle_buffer_size, batch_size)\n",
        "    print(\"data retrieved\")\n",
        "    #create model and tokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "    distilBert = TFNQModel.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "    print(\"Model created\")\n",
        "    if training_mode:\n",
        "        #get checkpoint if exists\n",
        "        if use_chkpt:\n",
        "            ckpt_manager = checkpt(distilBert, checkpoint_path)\n",
        "        else:\n",
        "            ckpt = tf.train.Checkpoint(model=distilBert)\n",
        "            ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
        "        #define accuracy and loss metrics\n",
        "        train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type = initialize_acc()\n",
        "        #create optimizer\n",
        "        optimizer = create_optimizer(distilBert)\n",
        "        #train\n",
        "        print(\"Training...\")\n",
        "        train(distilBert, optimizer, train_dataset, ckpt_manager, train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type)\n",
        "    else:\n",
        "        ckpt_manager = checkpt(distilBert, checkpoint_path)\n",
        "        print(\"Getting predictions...\")\n",
        "        #generate predictions.json by converting logits to labels\n",
        "        get_prediction_json(distilBert, val_dataset, pred_file, val_file, json_output_path, best_indexes)\n",
        "\n",
        "\n",
        "#     app.run(main)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "data retrieved\n",
            "Model created\n",
            "8\n",
            "Latest checkpoint restored\n",
            "Getting predictions...\n",
            "/content/drive/My Drive/Colab Notebooks/nq-dev-??.jsonl.gz\n",
            "/content/drive/My Drive/Colab Notebooks/predictions.json\n",
            "Batch 100 processed\n",
            "Batch 200 processed\n",
            "Batch 300 processed\n",
            "Batch 400 processed\n",
            "Batch 500 processed\n",
            "Batch 600 processed\n",
            "Batch 700 processed\n",
            "Batch 800 processed\n",
            "Batch 900 processed\n",
            "Batch 1000 processed\n",
            "Batch 1100 processed\n",
            "Batch 1200 processed\n",
            "Batch 1300 processed\n",
            "Batch 1400 processed\n",
            "Batch 1500 processed\n",
            "Batch 1600 processed\n",
            "Batch 1700 processed\n",
            "Batch 1800 processed\n",
            "Batch 1900 processed\n",
            "Batch 2000 processed\n",
            "Batch 2100 processed\n",
            "Batch 2200 processed\n",
            "Batch 2300 processed\n",
            "Batch 2400 processed\n",
            "Batch 2500 processed\n",
            "Batch 2600 processed\n",
            "Batch 2700 processed\n",
            "Batch 2800 processed\n",
            "Batch 2900 processed\n",
            "Batch 3000 processed\n",
            "Batch 3100 processed\n",
            "Batch 3200 processed\n",
            "Batch 3300 processed\n",
            "Batch 3400 processed\n",
            "Batch 3500 processed\n",
            "Batch 3600 processed\n",
            "Batch 3700 processed\n",
            "Batch 3800 processed\n",
            "Batch 3900 processed\n",
            "Batch 4000 processed\n",
            "Batch 4100 processed\n",
            "Batch 4200 processed\n",
            "Batch 4300 processed\n",
            "Batch 4400 processed\n",
            "Batch 4500 processed\n",
            "Batch 4600 processed\n",
            "Batch 4700 processed\n",
            "Batch 4800 processed\n",
            "Batch 4900 processed\n",
            "Batch 5000 processed\n",
            "Batch 5100 processed\n",
            "Batch 5200 processed\n",
            "Batch 5300 processed\n",
            "Batch 5400 processed\n",
            "Batch 5500 processed\n",
            "Batch 5600 processed\n",
            "Batch 5700 processed\n",
            "Batch 5800 processed\n",
            "Batch 5900 processed\n",
            "Batch 6000 processed\n",
            "Batch 6100 processed\n",
            "Batch 6200 processed\n",
            "Batch 6300 processed\n",
            "Batch 6400 processed\n",
            "Batch 6500 processed\n",
            "Batch 6600 processed\n",
            "Batch 6700 processed\n",
            "Batch 6800 processed\n",
            "Batch 6900 processed\n",
            "Batch 7000 processed\n",
            "Batch 7100 processed\n",
            "Batch 7200 processed\n",
            "Batch 7300 processed\n",
            "Batch 7400 processed\n",
            "Batch 7500 processed\n",
            "Batch 7600 processed\n",
            "Batch 7700 processed\n",
            "Batch 7800 processed\n",
            "Batch 7900 processed\n",
            "Batch 8000 processed\n",
            "Batch 8100 processed\n",
            "Batch 8200 processed\n",
            "Batch 8300 processed\n",
            "Batch 8400 processed\n",
            "Batch 8500 processed\n",
            "Batch 8600 processed\n",
            "Batch 8700 processed\n",
            "Batch 8800 processed\n",
            "Batch 8900 processed\n",
            "Batch 9000 processed\n",
            "Batch 9100 processed\n",
            "Batch 9200 processed\n",
            "Batch 9300 processed\n",
            "Batch 9400 processed\n",
            "Batch 9500 processed\n",
            "Batch 9600 processed\n",
            "Batch 9700 processed\n",
            "Batch 9800 processed\n",
            "Batch 9900 processed\n",
            "Batch 10000 processed\n",
            "Batch 10100 processed\n",
            "Batch 10200 processed\n",
            "Batch 10300 processed\n",
            "Batch 10400 processed\n",
            "Batch 10500 processed\n",
            "Batch 10600 processed\n",
            "Batch 10700 processed\n",
            "Batch 10800 processed\n",
            "Batch 10900 processed\n",
            "Batch 11000 processed\n",
            "Batch 11100 processed\n",
            "Batch 11200 processed\n",
            "Batch 11300 processed\n",
            "Batch 11400 processed\n",
            "Batch 11500 processed\n",
            "Batch 11600 processed\n",
            "Batch 11700 processed\n",
            "Batch 11800 processed\n",
            "Batch 11900 processed\n",
            "Batch 12000 processed\n",
            "Batch 12100 processed\n",
            "Batch 12200 processed\n",
            "Batch 12300 processed\n",
            "Batch 12400 processed\n",
            "Batch 12500 processed\n",
            "Batch 12600 processed\n",
            "Batch 12700 processed\n",
            "Batch 12800 processed\n",
            "Batch 12900 processed\n",
            "Batch 13000 processed\n",
            "Batch 13100 processed\n",
            "Batch 13200 processed\n",
            "Batch 13300 processed\n",
            "Batch 13400 processed\n",
            "Batch 13500 processed\n",
            "Batch 13600 processed\n",
            "Batch 13700 processed\n",
            "Batch 13800 processed\n",
            "Batch 13900 processed\n",
            "Batch 14000 processed\n",
            "Batch 14100 processed\n",
            "Batch 14200 processed\n",
            "Batch 14300 processed\n",
            "Batch 14400 processed\n",
            "Batch 14500 processed\n",
            "Batch 14600 processed\n",
            "Batch 14700 processed\n",
            "Batch 14800 processed\n",
            "Batch 14900 processed\n",
            "Batch 15000 processed\n",
            "Batch 15100 processed\n",
            "Batch 15200 processed\n",
            "Batch 15300 processed\n",
            "Batch 15400 processed\n",
            "Batch 15500 processed\n",
            "Batch 15600 processed\n",
            "Batch 15700 processed\n",
            "Batch 15800 processed\n",
            "Batch 15900 processed\n",
            "Batch 16000 processed\n",
            "Batch 16100 processed\n",
            "Batch 16200 processed\n",
            "Batch 16300 processed\n",
            "Batch 16400 processed\n",
            "Batch 16500 processed\n",
            "Batch 16600 processed\n",
            "Batch 16700 processed\n",
            "Batch 16800 processed\n",
            "Batch 16900 processed\n",
            "Batch 17000 processed\n",
            "Batch 17100 processed\n",
            "Batch 17200 processed\n",
            "Batch 17300 processed\n",
            "Batch 17400 processed\n",
            "Batch 17500 processed\n",
            "Batch 17600 processed\n",
            "Batch 17700 processed\n",
            "Batch 17800 processed\n",
            "Batch 17900 processed\n",
            "Batch 18000 processed\n",
            "Batch 18100 processed\n",
            "Batch 18200 processed\n",
            "Batch 18300 processed\n",
            "Batch 18400 processed\n",
            "Batch 18500 processed\n",
            "Batch 18600 processed\n",
            "Batch 18700 processed\n",
            "Batch 18800 processed\n",
            "Batch 18900 processed\n",
            "Batch 19000 processed\n",
            "Batch 19100 processed\n",
            "Batch 19200 processed\n",
            "Batch 19300 processed\n",
            "Batch 19400 processed\n",
            "Batch 19500 processed\n",
            "Batch 19600 processed\n",
            "Batch 19700 processed\n",
            "Batch 19800 processed\n",
            "Batch 19900 processed\n",
            "Batch 20000 processed\n",
            "Batch 20100 processed\n",
            "Batch 20200 processed\n",
            "Batch 20300 processed\n",
            "Batch 20400 processed\n",
            "Batch 20500 processed\n",
            "Batch 20600 processed\n",
            "Batch 20700 processed\n",
            "Batch 20800 processed\n",
            "Batch 20900 processed\n",
            "Batch 21000 processed\n",
            "Batch 21100 processed\n",
            "Batch 21200 processed\n",
            "Batch 21300 processed\n",
            "Batch 21400 processed\n",
            "Batch 21500 processed\n",
            "Batch 21600 processed\n",
            "Batch 21700 processed\n",
            "Batch 21800 processed\n",
            "Batch 21900 processed\n",
            "Batch 22000 processed\n",
            "Batch 22100 processed\n",
            "Batch 22200 processed\n",
            "Batch 22300 processed\n",
            "Batch 22400 processed\n",
            "Batch 22500 processed\n",
            "Batch 22600 processed\n",
            "Batch 22700 processed\n",
            "Batch 22800 processed\n",
            "Batch 22900 processed\n",
            "Batch 23000 processed\n",
            "Batch 23100 processed\n",
            "Batch 23200 processed\n",
            "Batch 23300 processed\n",
            "Batch 23400 processed\n",
            "Batch 23500 processed\n",
            "Batch 23600 processed\n",
            "Batch 23700 processed\n",
            "Batch 23800 processed\n",
            "Batch 23900 processed\n",
            "Batch 24000 processed\n",
            "Batch 24100 processed\n",
            "Batch 24200 processed\n",
            "Batch 24300 processed\n",
            "Batch 24400 processed\n",
            "Batch 24500 processed\n",
            "Batch 24600 processed\n",
            "Batch 24700 processed\n",
            "Batch 24800 processed\n",
            "Batch 24900 processed\n",
            "Batch 25000 processed\n",
            "Batch 25100 processed\n",
            "Batch 25200 processed\n",
            "Batch 25300 processed\n",
            "Batch 25400 processed\n",
            "Batch 25500 processed\n",
            "Batch 25600 processed\n",
            "Batch 25700 processed\n",
            "Batch 25800 processed\n",
            "Batch 25900 processed\n",
            "Batch 26000 processed\n",
            "Batch 26100 processed\n",
            "Batch 26200 processed\n",
            "Batch 26300 processed\n",
            "Batch 26400 processed\n",
            "Batch 26500 processed\n",
            "Batch 26600 processed\n",
            "Batch 26700 processed\n",
            "Batch 26800 processed\n",
            "Batch 26900 processed\n",
            "Batch 27000 processed\n",
            "Batch 27100 processed\n",
            "Batch 27200 processed\n",
            "Batch 27300 processed\n",
            "Batch 27400 processed\n",
            "Batch 27500 processed\n",
            "Batch 27600 processed\n",
            "Batch 27700 processed\n",
            "Batch 27800 processed\n",
            "Batch 27900 processed\n",
            "Batch 28000 processed\n",
            "Batch 28100 processed\n",
            "Batch 28200 processed\n",
            "Batch 28300 processed\n",
            "Batch 28400 processed\n",
            "Batch 28500 processed\n",
            "Batch 28600 processed\n",
            "Batch 28700 processed\n",
            "Batch 28800 processed\n",
            "Batch 28900 processed\n",
            "Batch 29000 processed\n",
            "Batch 29100 processed\n",
            "Batch 29200 processed\n",
            "Batch 29300 processed\n",
            "Batch 29400 processed\n",
            "Batch 29500 processed\n",
            "Batch 29600 processed\n",
            "Batch 29700 processed\n",
            "Batch 29800 processed\n",
            "Batch 29900 processed\n",
            "Batch 30000 processed\n",
            "Batch 30100 processed\n",
            "Batch 30200 processed\n",
            "Batch 30300 processed\n",
            "Batch 30400 processed\n",
            "Batch 30500 processed\n",
            "Batch 30600 processed\n",
            "Batch 30700 processed\n",
            "Batch 30800 processed\n",
            "Batch 30900 processed\n",
            "Batch 31000 processed\n",
            "Batch 31100 processed\n",
            "Batch 31200 processed\n",
            "Batch 31300 processed\n",
            "Batch 31400 processed\n",
            "Batch 31500 processed\n",
            "Batch 31600 processed\n",
            "Batch 31700 processed\n",
            "Batch 31800 processed\n",
            "Batch 31900 processed\n",
            "Batch 32000 processed\n",
            "Batch 32100 processed\n",
            "Batch 32200 processed\n",
            "Batch 32300 processed\n",
            "Batch 32400 processed\n",
            "Batch 32500 processed\n",
            "Batch 32600 processed\n",
            "Batch 32700 processed\n",
            "Batch 32800 processed\n",
            "Batch 32900 processed\n",
            "Batch 33000 processed\n",
            "Batch 33100 processed\n",
            "Batch 33200 processed\n",
            "Batch 33300 processed\n",
            "Batch 33400 processed\n",
            "Batch 33500 processed\n",
            "Batch 33600 processed\n",
            "Batch 33700 processed\n",
            "Batch 33800 processed\n",
            "Batch 33900 processed\n",
            "Batch 34000 processed\n",
            "Batch 34100 processed\n",
            "Batch 34200 processed\n",
            "Batch 34300 processed\n",
            "Batch 34400 processed\n",
            "Batch 34500 processed\n",
            "Batch 34600 processed\n",
            "Batch 34700 processed\n",
            "Batch 34800 processed\n",
            "Batch 34900 processed\n",
            "Batch 35000 processed\n",
            "Batch 35100 processed\n",
            "Batch 35200 processed\n",
            "Batch 35300 processed\n",
            "Batch 35400 processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqlKusrEW9k",
        "colab_type": "code",
        "outputId": "c7f65868-c6c2-4c56-fdb7-ba133e213f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlh8HoYlEQnt",
        "colab_type": "code",
        "outputId": "b777fa6a-dc8d-4bf8-fa6a-f32642e473f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ckpt = tf.train.Checkpoint(model=distilBert)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, \"./\", max_to_keep=10)\n",
        "ckpt_manager.save()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./ckpt-1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pyyfYoHEQnw",
        "colab_type": "code",
        "outputId": "ac3cf43c-1098-4598-e6fc-d8fcca2506c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!python -m nq_eval --gold_path=/content/drive/My\\ Drive/Colab\\ Notebooks/nq-dev-sample.jsonl.gz --predictions_path=/content/drive/My\\ Drive/Colab\\ Notebooks/predictions.json --logtostderr"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --gold_path has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n",
            "  'command line!' % flag_name)\n",
            "/usr/local/lib/python3.6/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --predictions_path has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n",
            "  'command line!' % flag_name)\n",
            "I0603 12:45:20.294454 140238757410688 eval_utils.py:260] parsing /content/drive/My Drive/Colab Notebooks/nq-dev-sample.jsonl.gz ..... \n",
            "I0603 12:45:23.101317 140238757410688 eval_utils.py:213] Reading predictions from file: /content/drive/My Drive/Colab Notebooks/predictions.json\n",
            "{\"long-best-threshold-f1\": 0.5803108808290156, \"long-best-threshold-precision\": 0.6222222222222222, \"long-best-threshold-recall\": 0.5436893203883495, \"long-best-threshold\": 6.31158971786499, \"long-recall-at-precision>=0.5\": 0.6504854368932039, \"long-precision-at-precision>=0.5\": 0.5037593984962406, \"long-recall-at-precision>=0.75\": 0.34951456310679613, \"long-precision-at-precision>=0.75\": 0.75, \"long-recall-at-precision>=0.9\": 0.009708737864077669, \"long-precision-at-precision>=0.9\": 1.0, \"short-best-threshold-f1\": 0.4729729729729729, \"short-best-threshold-precision\": 0.4794520547945205, \"short-best-threshold-recall\": 0.4666666666666667, \"short-best-threshold\": 6.9500908851623535, \"short-recall-at-precision>=0.5\": 0.37333333333333335, \"short-precision-at-precision>=0.5\": 0.5185185185185185, \"short-recall-at-precision>=0.75\": 0, \"short-precision-at-precision>=0.75\": 0, \"short-recall-at-precision>=0.9\": 0, \"short-precision-at-precision>=0.9\": 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwk7KkFqEQn5",
        "colab_type": "code",
        "outputId": "46db85e9-d006-432e-f6a8-ac76748dc905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !pip install tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIjUt7j0EQn7",
        "colab_type": "code",
        "outputId": "819fc155-58ec-477e-d468-118cea2ced9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun  2 20:43:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    71W / 149W |   1179MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnTlEi58EQn-",
        "colab_type": "code",
        "outputId": "666f88b9-b1b3-48cc-ebf5-e68d0e52953d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!ps -aux|grep python"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root          25  0.1  0.6 413020 84228 ?        Sl   14:17   0:32 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --MappingKernelManager.root_dir=\"/content\"\n",
            "root        7106 21.6 20.4 44274496 2724444 ?    Ssl  20:17   3:36 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5a1ccf58-b03f-41c5-a53b-edf78f302b32.json\n",
            "root        7933  0.0  0.0  39196  6504 ?        S    20:33   0:00 /bin/bash -c ps -aux|grep python\n",
            "root        7935  0.0  0.0  38568  4916 ?        S    20:33   0:00 grep python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_MZNvW1qZMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[1]*10**10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqE3L16wqbbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wzVNN_Azy1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "28b6c139-d2df-410e-a333-8050a30e956b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyyJ7zyAz0Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}